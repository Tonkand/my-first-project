import spacy
import pandas as pd
from pyspark.sql import SparkSession

# 初始化 Spark
spark = SparkSession.builder.appName("NLP Flagging with spaCy.pipe").getOrCreate()

# 測試資料（Spark 轉 Pandas）
spark_df = spark.createDataFrame([
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",)
], ["text"])

pdf = spark_df.toPandas()

# 初始化 spaCy（僅一次）
nlp = spacy.load("en_core_web_sm")

# 自定義邏輯設定
raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]
flag_label = "cata"
custom_mode = "keyword"
require_both_keyword_and_noun = False
ORG_mode = True
hours_pattern = r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b'

# 創建規則
def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

match_rules = create_match_rules(raw_keywords, noun_terms)
excluded_subjects = {"government", "ministry", "department", "agency", "council"}
fallback_orgs = {"apple", "tesla", "google", "amazon", "microsoft"}

results = []

for doc, row in zip(nlp.pipe(pdf["text"].tolist()), pdf.itertuples()):
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]
    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]

    important_indices = []
    match_keyword = False
    match_noun = False
    match_org = False

    for text_tok, lemma, pos, _, idx in tokens:
        for rule in match_rules:
            if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                important_indices.append(idx)
                match_noun = True

    for rule in match_rules:
        if rule["type"] == "phrasal":
            for i in range(len(tokens) - 1):
                if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                    important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                    match_keyword = True

    if ORG_mode:
        for org_text, label, start in ents:
            if label == "ORG" and org_text.lower() not in excluded_subjects:
                match_org = True
                important_indices.append(start)
        if not match_org:
            for token in token_texts:
                if token in fallback_orgs:
                    match_org = True

    has_excluded_subject = not match_noun and any(
        lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
        for _, lemma, _, dep, _ in tokens
    )

    if custom_mode == "year":
        time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", row.text)
    elif custom_mode == "hour":
        time_matches = re.findall(hours_pattern, row.text, re.IGNORECASE)
        time_matches = [t.strip() for t in time_matches]
    elif custom_mode == "keyword":
        time_matches = [kw for kw in custom_keywords if kw in row.text.lower()]
    else:
        time_matches = []

    closest_time = None
    if time_matches and important_indices:
        def distance(t):
            return min(
                abs(tk.i - idx)
                for idx in important_indices
                for tk in doc
                if tk.text.lower() in t.lower().split()
            ) if any(w in row.text.lower() for w in t.lower().split()) else float('inf')
        closest_time = min(time_matches, key=distance)

    voice = "unknown"
    for _, _, _, dep, _ in tokens:
        if dep == "nsubjpass":
            voice = "passive"
            break
        elif dep == "nsubj":
            voice = "active"

    if require_both_keyword_and_noun:
        passed = match_keyword and (match_noun or match_org)
    else:
        passed = match_keyword or match_noun or (match_org if ORG_mode else False)

    if passed and time_matches and not has_excluded_subject:
        results.append((row.text, flag_label, closest_time, voice, time_matches[0]))
    else:
        results.append((row.text, "", None, voice, None))

# 結果轉 Spark DataFrame 顯示
columns = ["text", "flag", "closest_term", "voice", "matched_term"]
final_df = spark.createDataFrame(pd.DataFrame(results, columns=columns))

import ace_tools as tools; tools.display_dataframe_to_user(name="Long Text NLP Result", dataframe=final_df.toPandas())

#####version 2
import re
import spacy
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark and spaCy
spark = SparkSession.builder.appName("Full NLP Pipeline with spaCy").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# Define rules
raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]
flag_label = "cata"
custom_mode = "year"  # 'year', 'hour', 'keyword'
require_both_keyword_and_noun = False
ORG_mode = True
hours_pattern = r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b'

# Create match rules
def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

# Broadcast configurations
match_rules = create_match_rules(raw_keywords, noun_terms)
br_match_rules = spark.sparkContext.broadcast(match_rules)
br_flag_label = spark.sparkContext.broadcast(flag_label)
br_custom_keywords = spark.sparkContext.broadcast(custom_keywords)
br_custom_mode = spark.sparkContext.broadcast(custom_mode)
br_hours_pattern = spark.sparkContext.broadcast(hours_pattern)
br_ORG_mode = spark.sparkContext.broadcast(ORG_mode)
br_require_both = spark.sparkContext.broadcast(require_both_keyword_and_noun)

# Define UDF
def analyze_text(text):
    if not text:
        return ("", None, "unknown", None)

    doc = nlp(text)
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]

    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]

    match_rules = br_match_rules.value
    custom_keywords = br_custom_keywords.value
    flag_label = br_flag_label.value
    require_both = br_require_both.value
    ORG_mode = br_ORG_mode.value
    custom_mode = br_custom_mode.value
    hours_pattern = br_hours_pattern.value

    important_indices = []
    match_keyword = False
    match_noun = False
    match_org = False
    excluded_subjects = {"government", "ministry", "department", "agency", "council"}

    for text_tok, lemma, pos, _, idx in tokens:
        for rule in match_rules:
            if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                important_indices.append(idx)
                match_noun = True

    for rule in match_rules:
        if rule["type"] == "phrasal":
            for i in range(len(tokens) - 1):
                if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                    important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                    match_keyword = True

    if ORG_mode:
        for org_text, label, start in ents:
            if label == "ORG" and org_text.lower() not in excluded_subjects:
                match_org = True
                important_indices.append(start)
        if not match_org:
            fallback_orgs = ["apple", "microsoft", "tesla", "amazon", "google"]
            for token in token_texts:
                if token in fallback_orgs:
                    match_org = True

    has_excluded_subject = not match_noun and any(
        lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
        for _, lemma, _, dep, _ in tokens
    )

    if custom_mode == "year":
        time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", text)
        time_matches += re.findall(r"\b\d{1,2}/\d{1,2}/\d{4}\b", text)
    elif custom_mode == "hour":
        time_matches = re.findall(hours_pattern, text, re.IGNORECASE)
        time_matches = [''.join(x).strip() if isinstance(x, tuple) else x.strip() for x in time_matches]
    elif custom_mode == "keyword":
        time_matches = [kw for kw in custom_keywords if kw in text.lower()]
    else:
        time_matches = []

    has_time = bool(time_matches)

    closest_time = None
    if time_matches and important_indices:
        def safe_distance(t):
            distances = [
                abs(token.i - idx)
                for idx in important_indices
                for token in doc
                if token.text.lower() in t.lower().split()
            ]
            return min(distances) if distances else float('inf')

        closest_time = min(time_matches, key=safe_distance)

    voice = "unknown"
    for _, _, _, dep, _ in tokens:
        if dep == "nsubjpass":
            voice = "passive"
            break
        elif dep == "nsubj":
            voice = "active"

    if require_both:
        passed = match_keyword and (match_noun or match_org) if ORG_mode else match_keyword and match_noun
    else:
        passed = match_keyword or match_noun or (match_org if ORG_mode else False)

    if passed and has_time and not has_excluded_subject:
        return (flag_label, closest_time, voice, time_matches[0] if time_matches else None)
    else:
        return ("", None, voice, time_matches[0] if time_matches else None)

schema = StructType([
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType()),
    StructField("matched_term", StringType())
])

analyze_udf = udf(analyze_text, schema)

# Test data
df = spark.createDataFrame([
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",),
    ("U.S. FDA through 12/31/2023.",)
], ["text"])

df_result = df.withColumn("result", analyze_udf(col("text"))) \
    .select("text", "result.flag", "result.closest_term", "result.voice", "result.matched_term")

import ace_tools as tools; tools.display_dataframe_to_user(name="Fixed NLP Result", dataframe=df_result.toPandas())



##################
# Re-run after kernel reset
import spacy
import re
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark
spark = SparkSession.builder.appName("NLP Pipeline Safe Driver Pipe").getOrCreate()

# Sample input data
input_data = [
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",),
    ("U.S. FDA through 12/31/2023.",)
]

# Convert to Spark DataFrame and collect to driver
df = spark.createDataFrame(input_data, ["text"])
rows = df.collect()

# Load spaCy on driver
import spacy
nlp = spacy.load("en_core_web_sm")

# Define logic configuration
raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]
flag_label = "cata"
custom_mode = "keyword"  # Options: 'year', 'hour', 'keyword'
require_both_keyword_and_noun = False
ORG_mode = True
hours_pattern = r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b'

# Compile rules
def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

match_rules = create_match_rules(raw_keywords, noun_terms)
excluded_subjects = {"government", "ministry", "department", "agency", "council"}
fallback_orgs = {"apple", "tesla", "google", "amazon", "microsoft"}

# Process with spaCy.pipe on driver
results = []
for doc, row in zip(nlp.pipe([r.text for r in rows]), rows):
    text = row.text
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]
    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]

    important_indices = []
    match_keyword = match_noun = match_org = False

    for text_tok, lemma, pos, _, idx in tokens:
        for rule in match_rules:
            if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                important_indices.append(idx)
                match_noun = True

    for rule in match_rules:
        if rule["type"] == "phrasal":
            for i in range(len(tokens) - 1):
                if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                    important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                    match_keyword = True

    if ORG_mode:
        for org_text, label, start in ents:
            if label == "ORG" and org_text.lower() not in excluded_subjects:
                match_org = True
                important_indices.append(start)
        if not match_org:
            for token in token_texts:
                if token in fallback_orgs:
                    match_org = True

    has_excluded_subject = not match_noun and any(
        lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
        for _, lemma, _, dep, _ in tokens
    )

    if custom_mode == "year":
        time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", text)
        time_matches += re.findall(r"\b\d{1,2}/\d{1,2}/\d{4}\b", text)
    elif custom_mode == "hour":
        time_matches = re.findall(hours_pattern, text, re.IGNORECASE)
        time_matches = [t.strip() for t in time_matches]
    elif custom_mode == "keyword":
        time_matches = [kw for kw in custom_keywords if kw in text.lower()]
    else:
        time_matches = []

    has_time = bool(time_matches)

    closest_time = None
    if time_matches and important_indices:
        def safe_distance(t):
            distances = [
                abs(token.i - idx)
                for idx in important_indices
                for token in doc
                if token.text.lower() in t.lower().split()
            ]
            return min(distances) if distances else float('inf')
        closest_time = min(time_matches, key=safe_distance)

    voice = "unknown"
    for _, _, _, dep, _ in tokens:
        if dep == "nsubjpass":
            voice = "passive"
            break
        elif dep == "nsubj":
            voice = "active"

    if require_both_keyword_and_noun:
        passed = match_keyword and (match_noun or match_org)
    else:
        passed = match_keyword or match_noun or (match_org if ORG_mode else False)

    if passed and has_time and not has_excluded_subject:
        results.append((text, flag_label, closest_time, voice, time_matches[0]))
    else:
        results.append((text, "", None, voice, None))

# Return Spark DataFrame
schema = StructType([
    StructField("text", StringType(), True),
    StructField("flag", StringType(), True),
    StructField("closest_term", StringType(), True),
    StructField("voice", StringType(), True),
    StructField("matched_term", StringType(), True),
])

final_df = spark.createDataFrame(results, schema=schema)

import ace_tools as tools; tools.display_dataframe_to_user(name="Driver-Based NLP Result", dataframe=final_df.toPandas())


######
import re
import spacy
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, udf, col
from pyspark.sql.types import StructType, StructField, StringType, ArrayType

# 初始化 Spark 和 spaCy
spark = SparkSession.builder.appName("Sentence-Level NLP").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# 自訂參數
raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]
flag_label = "cata"
custom_mode = "year"
require_both_keyword_and_noun = False
ORG_mode = True
hours_pattern = r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b'
excluded_subjects = {"government", "ministry", "department", "agency", "council"}
fallback_orgs = {"apple", "tesla", "google", "amazon", "microsoft"}

def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

match_rules = create_match_rules(raw_keywords, noun_terms)

def analyze_sentences(text):
    if not text:
        return []

    doc = nlp(text)
    results = []

    for sent in doc.sents:
        sentence = sent.text.strip()
        if not sentence:
            continue

        doc_sent = nlp(sentence)
        tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc_sent]
        ents = [(e.text, e.label_, e.start) for e in doc_sent.ents]
        token_texts = [t[0].lower() for t in tokens]
        lemmas = [t[1].lower() for t in tokens]

        important_indices = []
        match_keyword = False
        match_noun = False
        match_org = False

        for text_tok, lemma, pos, _, idx in tokens:
            for rule in match_rules:
                if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                    important_indices.append(idx)
                    match_keyword = True
                elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                    important_indices.append(idx)
                    match_keyword = True
                elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                    important_indices.append(idx)
                    match_noun = True

        for rule in match_rules:
            if rule["type"] == "phrasal":
                for i in range(len(tokens) - 1):
                    if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                        important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                        match_keyword = True

        if ORG_mode:
            for org_text, label, start in ents:
                if label == "ORG" and org_text.lower() not in excluded_subjects:
                    match_org = True
                    important_indices.append(start)
            if not match_org:
                for token in token_texts:
                    if token in fallback_orgs:
                        match_org = True

        has_excluded_subject = not match_noun and any(
            lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
            for _, lemma, _, dep, _ in tokens
        )

        if custom_mode == "year":
            time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", sentence)
            time_matches += re.findall(r"\b\d{1,2}/\d{1,2}/\d{4}\b", sentence)
        elif custom_mode == "hour":
            time_matches = re.findall(hours_pattern, sentence, re.IGNORECASE)
            time_matches = [t.strip() for t in time_matches]
        elif custom_mode == "keyword":
            time_matches = [kw for kw in custom_keywords if kw in sentence.lower()]
        else:
            time_matches = []

        closest_time = None
        if time_matches and important_indices:
            def safe_distance(t):
                distances = [
                    abs(token.i - idx)
                    for idx in important_indices
                    for token in doc_sent
                    if token.text.lower() in t.lower().split()
                ]
                return min(distances) if distances else float('inf')
            closest_time = min(time_matches, key=safe_distance)

        voice = "unknown"
        for _, _, _, dep, _ in tokens:
            if dep == "nsubjpass":
                voice = "passive"
                break
            elif dep == "nsubj":
                voice = "active"

        if require_both_keyword_and_noun:
            passed = match_keyword and (match_noun or match_org)
        else:
            passed = match_keyword or match_noun or (match_org if ORG_mode else False)

        if passed and time_matches and not has_excluded_subject:
            results.append((sentence, flag_label, closest_time, voice, time_matches[0]))
        else:
            results.append((sentence, "", None, voice, time_matches[0] if time_matches else None))

    return results

# 註冊 UDF
schema = ArrayType(StructType([
    StructField("sentence", StringType()),
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType()),
    StructField("matched_term", StringType())
]))
analyze_udf = udf(analyze_sentences, schema)

# 測試資料
df = spark.createDataFrame([
    ("We will acquire the company as soon as possible. The firm bought the site in 2020. Government took possession without notice.",)
], ["text"])

df_result = df.withColumn("sentences", analyze_udf(col("text"))) \
              .selectExpr("explode(sentences) as result") \
              .selectExpr("result.sentence", "result.flag", "result.closest_term", "result.voice", "result.matched_term")

import ace_tools as tools; tools.display_dataframe_to_user(name="Sentence-Level NLP Result", dataframe=df_result.toPandas())
