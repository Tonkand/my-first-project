from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("CreateDF").getOrCreate()

data = [
    (1, "Alice", 30),
    (2, "Bob", 28),
    (3, "Cathy", 35)
]

columns = ["id", "name", "age"]

df = spark.createDataFrame(data, schema=columns)
df.show()


####show rows
for row in df.toLocalIterator():
    for col in df.columns:
        print(f"{col}: {row[col]}")
    print("-----")


####convert to spark
from pyspark.sql import SparkSession
import pandas as pd

# 初始化 SparkSession
spark = SparkSession.builder.appName("PandasToSpark").getOrCreate()

# 创建 pandas DataFrame
pdf = pd.DataFrame({
    "id": [1, 2],
    "name": ["Alice", "Bob"]
})

#####filters

df_filtered = df.filter(df["amount"] != 0)
from pyspark.sql.functions import col
df_filtered = df.filter((col("amount") != 0) & (col("count") != 0))


###多個
non_zero_cols = ["a", "b", "c"]
from functools import reduce
condition = reduce(lambda x, y: x | (col(y) != 0), non_zero_cols[1:], (col(non_zero_cols[0]) != 0))


df_filtered = df.filter(condition)

# 转换为 Spark DataFrame
df = spark.createDataFrame(pdf)
df.show()
