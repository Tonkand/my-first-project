from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("CreateDF").getOrCreate()

data = [
    (1, "Alice", 30),
    (2, "Bob", 28),
    (3, "Cathy", 35)
]

columns = ["id", "name", "age"]

df = spark.createDataFrame(data, schema=columns)
df.show()


####show rows
for row in df.toLocalIterator():
    for col in df.columns:
        print(f"{col}: {row[col]}")
    print("-----")
