import spacy
import re
import pandas as pd

nlp = spacy.load("en_core_web_sm")

def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + noun_terms
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

def analyze_text_with_flags(df, columns_name, raw_keywords, noun_terms, flag_label,
                             ORG_mode=True, require_both_keyword_and_noun=True,
                             custom_mode='year',
                             hours_pattern=r'\b\d{1,2}(:\d{2})?\s*(am|pm)\b',
                             custom_keywords=None):
    
    match_rules = create_match_rules(raw_keywords, noun_terms)
    
    # Extract time-matching tokens
    if custom_mode == 'year':
        df['time_matches'] = df[columns_name].str.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b")
        df['matched_term'] = df['time_matches'].apply(lambda x: x[0] if x else None)
    elif custom_mode == 'hour':
        df['time_matches'] = df[columns_name].str.findall(hours_pattern)
        df['time_matches'] = df['time_matches'].apply(lambda lst: [''.join(x).strip() for x in lst])
        df['matched_term'] = df['time_matches'].apply(lambda x: x[0] if x else None)
    elif custom_mode == 'keyword':
        if not custom_keywords:
            raise ValueError("custom_keywords must be provided when custom_mode='keyword'")
        keywords_lower = [k.lower() for k in custom_keywords]
        def find_phrases(text):
            text_l = text.lower()
            matched = [kw for kw in keywords_lower if kw in text_l]
            return matched
        df['time_matches'] = df[columns_name].apply(find_phrases)
        df['matched_term'] = df['time_matches'].apply(lambda x: x[0] if x else None)
    else:
        raise ValueError("custom_mode must be one of: 'year', 'hour', 'keyword'")

    docs = list(nlp.pipe(df[columns_name].tolist()))
    df['spacy_data'] = [{
        'doc': doc,
        'token_info': [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc],
        'entity_info': [(e.text, e.label_, e.start) for e in doc.ents]
    } for doc in docs]
    df['token_info'] = df['spacy_data'].apply(lambda x: x['token_info'])
    df['entity_info'] = df['spacy_data'].apply(lambda x: x['entity_info'])

    def analyze_row(row):
        tokens = row['token_info']
        ents = row['entity_info']
        doc = row['spacy_data']['doc']
        time_matches = row['time_matches']
        has_time = bool(time_matches)

        token_texts = [t[0].lower() for t in tokens]
        lemmas = [t[1].lower() for t in tokens]

        important_indices = []
        match_keyword = False
        match_noun = False
        match_org = False

        for i, (text_tok, lemma, pos, _, idx) in enumerate(tokens):
            for rule in match_rules:
                if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                    important_indices.append(idx)
                    match_keyword = True
                elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                    important_indices.append(idx)
                    match_keyword = True
                elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                    important_indices.append(idx)
                    match_noun = True

        for rule in match_rules:
            if rule["type"] == "phrasal":
                for i in range(len(tokens) - 1):
                    if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                        important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                        match_keyword = True

        excluded_subjects = {"government", "ministry", "department", "agency", "council"}
        if ORG_mode:
            for org_text, label, start in ents:
                if label == "ORG" and org_text.lower() not in excluded_subjects:
                    match_org = True
                    important_indices.append(start)

            fallback_orgs = ["apple", "microsoft", "tesla", "amazon", "google"]
            if not match_org:
                for token in token_texts:
                    if token in fallback_orgs:
                        match_org = True

        if not match_noun:
            has_excluded_subject = any(
                lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
                for _, lemma, _, dep, _ in tokens
            )
        else:
            has_excluded_subject = False

        closest_time = None
        if time_matches and important_indices:
            closest_time = min(
                time_matches,
                key=lambda t: min(
                    [abs(token.i - idx) for idx in important_indices for token in doc if token.text.lower() in t],
                    default=float('inf')
                )
            )

        voice = "unknown"
        for _, _, _, dep, _ in tokens:
            if dep == "nsubjpass":
                voice = "passive"
                break
            elif dep == "nsubj":
                voice = "active"

        if require_both_keyword_and_noun:
            if ORG_mode:
                passed = match_keyword and (match_noun or match_org)
            else:
                passed = match_keyword and match_noun
        else:
            passed = match_keyword or match_noun or (match_org if ORG_mode else False)

        if passed and has_time and not has_excluded_subject:
            return flag_label, closest_time, voice

        return "", None, voice

    time_col = {
        'year': 'closest_year',
        'hour': 'closest_hour',
        'keyword': 'closest_term'
    }[custom_mode]

    df[['flag', time_col, 'voice']] = df.apply(analyze_row, axis=1, result_type='expand')
    return df

# 測試資料
df = pd.DataFrame({
    'text': [
        "We will acquire the company as soon as possible.",
        "The firm bought the site in 2020.",
        "Tesla gained control right away and expanded later.",
        "They will act immediately to take over.",
        "Government took possession without notice."
    ]
})

# 測試：支援片語 + 顯示哪個詞匹配
df_result = analyze_text_with_flags(
    df, 'text',
    raw_keywords=["acquire", "gain", "purchase", "buy", "take over"],
    noun_terms=["firm", "startup", "company", "site"],
    flag_label="cata",
    ORG_mode=True,
    require_both_keyword_and_noun=False,
    custom_mode='keyword',
    custom_keywords=["soon", "immediately", "right away", "as soon as possible", "later"]
)

print(df_result[['text', 'flag', 'closest_term', 'matched_term', 'voice']])
