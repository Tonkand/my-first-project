import pandas as pd
import spacy
from pyspark.sql.functions import pandas_udf, PandasUDFType, col, explode
from pyspark.sql import SparkSession

# 載入 spaCy 模型（關閉不必要模組加快速度）
nlp = spacy.load("en_core_web_sm", disable=["ner", "tagger", "parser"])
if not nlp.has_pipe("sentencizer"):
    nlp.add_pipe("sentencizer")

# 多分類關鍵字群組
keyword_groups = {
    'company_terms': ['firm', 'company'],
    'location_terms': ['office', 'headquarters'],
    'product_terms': ['device']
}

# 預先轉成小寫 set，加速查找
lowered_keyword_groups = {
    cat: set([kw.lower() for kw in kws]) for cat, kws in keyword_groups.items()
}

# 定義 pandas_udf：針對每個 cell 回傳多組 match（句子+分類+關鍵字）
@pandas_udf("array<struct<sentence:string, category:string, keyword:string>>", PandasUDFType.SCALAR)
def extract_sentences_keywords_udf(texts: pd.Series) -> pd.Series:
    results = []
    for doc in nlp.pipe(texts.fillna(""), batch_size=1000):
        matches = []
        for sent in doc.sents:
            sent_text = sent.text.strip()
            lower_sent = sent_text.lower()
            for category, kw_set in lowered_keyword_groups.items():
                for kw in kw_set:
                    if kw in lower_sent:
                        matches.append({
                            "sentence": sent_text,
                            "category": category,
                            "keyword": kw
                        })
        results.append(matches)
    return pd.Series(results)

# 建立範例 Spark DataFrame
df_spark = spark.createDataFrame([
    ("The firm relocated its headquarters.",),
    ("This device was approved by the company in our office.",),
    ("Nothing interesting here.",)
], ["text"])

# 套用 UDF 並展平為多列
df_result = df_spark.withColumn("keyword_matches", extract_sentences_keywords_udf(col("text"))) \
                    .withColumn("match", explode("keyword_matches")) \
                    .selectExpr("match.sentence", "match.category", "match.keyword")

display(df_result)


#########easy one

import spacy

# 載入 spaCy 並啟用 sentencizer（更快）
nlp = spacy.load("en_core_web_sm", disable=["ner", "parser", "tagger"])
if not nlp.has_pipe("sentencizer"):
    nlp.add_pipe("sentencizer")

# 關鍵字定義
keyword_groups = {
    "company_terms": ["firm", "company"],
    "location_terms": ["office", "headquarters"],
    "product_terms": ["device"]
}

# 預轉換為小寫 + set，加快查找
lowered_keyword_groups = {
    cat: set(kw.lower() for kw in kws)
    for cat, kws in keyword_groups.items()
}

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

df = spark.createDataFrame([
    ("The firm relocated its headquarters.",),
    ("The company approved the new device in our office.",),
    ("Nothing interesting here.",)
], ["text"])

def extract_sentences_with_keywords(row):
    text = row['text']
    results = []
    if not text:
        return results
    
    doc = nlp(text)
    for sent in doc.sents:
        sent_text = sent.text.strip()
        sent_lower = sent_text.lower()
        for category, kw_set in lowered_keyword_groups.items():
            for kw in kw_set:
                if kw in sent_lower:
                    results.append((sent_text, category, kw))
    return results

# 用 rdd.flatMap 套用處理
results_rdd = df.rdd.flatMap(extract_sentences_with_keywords)

# 轉回 DataFrame
df_result = results_rdd.toDF(["sentence", "category", "keyword"])

# 顯示結果
display(df_result)


########pandas 版
from pyspark.sql import SparkSession
import pandas as pd
import spacy

# ✅ 建立 SparkSession（如果還沒建立）
spark = SparkSession.builder.getOrCreate()

# ✅ 假設原始資料
df_spark = spark.createDataFrame([
    ("The firm relocated its headquarters.",),
    ("The company approved the new device in our office.",),
    ("Nothing interesting here.",)
], ["text"])

# ✅ 轉成 pandas DataFrame
df = df_spark.toPandas()

# ✅ 載入 spaCy 並啟用 sentencizer
nlp = spacy.load("en_core_web_sm", disable=["ner", "tagger", "parser"])
if not nlp.has_pipe("sentencizer"):
    nlp.add_pipe("sentencizer")

# ✅ 定義關鍵字
keyword_groups = {
    "company_terms": ["firm", "company"],
    "location_terms": ["office", "headquarters"],
    "product_terms": ["device"]
}
lowered_keyword_groups = {
    cat: set(kw.lower() for kw in kws) for cat, kws in keyword_groups.items()
}

# ✅ 關鍵字比對邏輯
results = []

for text in df['text'].dropna():
    doc = nlp(text)
    for sent in doc.sents:
        sent_text = sent.text.strip()
        lower = sent_text.lower()
        matched = False  # 用來記錄這句有沒有匹配到關鍵字

        for cat, kws in lowered_keyword_groups.items():
            for kw in kws:
                if kw in lower:
                    results.append({
                        "sentence": sent_text,
                        "category": cat,
                        "keyword": kw
                    })
                    matched = True  # 有匹配

        # 如果沒有任何匹配，補上一行空欄位
        if not matched:
            results.append({
                "sentence": sent_text,
                "category": None,
                "keyword": None
            })


# ✅ 回存成 Spark DataFrame（可選）
df_result = pd.DataFrame(results)
df_result_spark = spark.createDataFrame(df_result)

display(df_result_spark)

######新方法
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType
import spacy

# 初始化 Spark
spark = SparkSession.builder.getOrCreate()

# 你的關鍵字分組
keyword_groups = {
    "company_terms": ["firm", "company"],
    "location_terms": ["office", "headquarters"],
    "product_terms": ["device"]
}
lowered_keyword_groups = {
    cat: set([kw.lower() for kw in kws]) for cat, kws in keyword_groups.items()
}

# 建立原始 DataFrame
df = spark.createDataFrame([
    ("The firm moved its headquarters.",),
    ("The company developed a new device in the office.",),
    ("Nothing important here.",)
], ["text"])

# 定義每個 partition 的處理邏輯
def process_partition(rows):
    import spacy
    nlp = spacy.load("en_core_web_sm", disable=["ner", "tagger", "parser"])
    if not nlp.has_pipe("sentencizer"):
        nlp.add_pipe("sentencizer")

    for row in rows:
        text = row['text']
        if text is None:
            continue
        doc = nlp(text)
        for sent in doc.sents:
            sent_text = sent.text.strip()
            sent_lower = sent_text.lower()
            matched = False
            for cat, kw_set in lowered_keyword_groups.items():
                for kw in kw_set:
                    if kw in sent_lower:
                        yield (sent_text, cat, kw)
                        matched = True
            if not matched:
                yield (sent_text, None, None)

# 轉換為 RDD 處理
rdd = df.rdd.mapPartitions(process_partition)

# 定義輸出 schema 並轉為 DataFrame
schema = StructType([
    StructField("sentence", StringType(), True),
    StructField("category", StringType(), True),
    StructField("keyword", StringType(), True)
])

df_result = spark.createDataFrame(rdd, schema=schema)

# 顯示結果
display(df_result)
