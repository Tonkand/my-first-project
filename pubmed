
import pandas as pd
import requests
from tqdm import tqdm

# 假設你的 dataframe 長這樣
df = pd.DataFrame({
    'drug_name': ['acetaminophen', 'ibuprofen', 'aspirin', 'hydroxyzine', 'notarealdrug']
})

# 查詢函數
def get_pubmed_hits(term):
    url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    params = {
        "db": "pubmed",
        "term": term,
        "retmode": "json"
    }
    try:
        r = requests.get(url, params=params)
        r.raise_for_status()
        data = r.json()
        return int(data["esearchresult"]["count"])
    except Exception as e:
        print(f"Error with term '{term}': {e}")
        return None

# 用 tqdm 顯示進度條
tqdm.pandas()

##########

import os
import gzip
import xml.etree.ElementTree as ET
from tqdm import tqdm
import pandas as pd

# ======= 請修改你的資料夾路徑（包含所有 .xml.gz） =======
PUBMED_FOLDER = "path/to/medline/files"  # ← 改成你儲存 PubMed XML 的資料夾

# 你要查詢的單詞（小寫）
target_words = ['acetaminophen', 'aspirin', 'ibuprofen', 'cancer']

# 統計字典：{word: 出現在幾篇文章中}
article_counts = {word: 0 for word in target_words}

# 所有 .gz 檔案路徑
files = [os.path.join(PUBMED_FOLDER, f) for f in os.listdir(PUBMED_FOLDER) if f.endswith('.xml.gz')]
print(f"Found {len(files)} PubMed files.")

# 主處理函數
def process_pubmed_file(file_path, article_counts):
    try:
        with gzip.open(file_path, 'rb') as f:
            tree = ET.parse(f)
            root = tree.getroot()
            for article in root.findall('.//PubmedArticle'):
                title = article.findtext('.//ArticleTitle') or ""
                abstract = article.findtext('.//AbstractText') or ""
                combined = (title + " " + abstract).lower()

                for word in target_words:
                    if word in combined:
                        article_counts[word] += 1
                        # 這篇文章中出現一個詞後就不重複計算該詞
                        # 可改成集合記錄避免重複統計
    except Exception as e:
        print(f"Failed to process {file_path}: {e}")

# 處理所有檔案
for file in tqdm(files, desc="Processing PubMed files"):
    process_pubmed_file(file, article_counts)

# 輸出結果為 DataFrame
df_result = pd.DataFrame([
    {"word": word, "article_count": count}
    for word, count in article_counts.items()
])

print(df_result)

# 可選：存為 CSV
df_result.to_csv("pubmed_word_article_counts.csv", index=False)


# 新增一欄：PubMed 點擊數（匹配數量）
df['pubmed_hits'] = df['drug_name'].progress_apply(get_pubmed_hits)

# 顯示結果
print(df)


#######

import pandas as pd
import requests
from tqdm import tqdm

# 假設的 DataFrame
df = pd.DataFrame({
    'drug_name': ['acetaminophen', 'ibuprofen', 'aspirin', 'hydroxyzine', 'notarealdrug']
})

# 查詢 PubChem 的函數
def get_pubchem_cid(term):
    url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{term}/cids/JSON"
    try:
        r = requests.get(url, timeout=5)
        r.raise_for_status()
        data = r.json()
        return data['IdentifierList']['CID'][0]  # 只取第一個 CID
    except Exception:
        return None  # 沒找到或出錯時返回 None

# 加上進度條
tqdm.pandas()

# 新增一欄，顯示是否存在於 PubChem（CID）
df['pubchem_cid'] = df['drug_name'].progress_apply(get_pubchem_cid)

# 顯示結果
print(df)



import os
import requests
import re
from tqdm import tqdm

# 设置下载目录
DOWNLOAD_DIR = "pubmed_baseline"
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# PubMed FTP 的 HTTP 映射地址
BASE_URL = "https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/"

# 获取所有 .xml.gz 文件列表
def get_pubmed_file_list():
    response = requests.get(BASE_URL)
    filenames = re.findall(r'pubmed\d+n\d+\.xml\.gz', response.text)
    return sorted(set(filenames))

# 下载单个文件
def download_file(filename):
    url = BASE_URL + filename
    local_path = os.path.join(DOWNLOAD_DIR, filename)

    if os.path.exists(local_path):
        return  # 文件已存在，跳过下载

    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        total_size = int(r.headers.get("content-length", 0))
        with open(local_path, "wb") as f, tqdm(
            desc=filename,
            total=total_size,
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as bar:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
                bar.update(len(chunk))

# 主程序
files_to_download = get_pubmed_file_list()
print(f"找到 {len(files_to_download)} 个 PubMed 文件。")

for filename in files_to_download:
    download_file(filename)

print("✅ 所有 PubMed XML 文件下载完成。")


#########
import os
import gzip
import xml.etree.ElementTree as ET
from tqdm import tqdm
import pandas as pd

# ======= 請修改你的資料夾路徑（包含所有 .xml.gz） =======
PUBMED_FOLDER = "path/to/medline/files"  # ← 改成你儲存 PubMed XML 的資料夾

# 你要查詢的單詞（小寫）
target_words = ['acetaminophen', 'aspirin', 'ibuprofen', 'cancer']

# 統計字典：{word: 出現在幾篇文章中}
article_counts = {word: 0 for word in target_words}

# 所有 .gz 檔案路徑
files = [os.path.join(PUBMED_FOLDER, f) for f in os.listdir(PUBMED_FOLDER) if f.endswith('.xml.gz')]
print(f"Found {len(files)} PubMed files.")

# 主處理函數
def process_pubmed_file(file_path, article_counts):
    try:
        with gzip.open(file_path, 'rb') as f:
            tree = ET.parse(f)
            root = tree.getroot()
            for article in root.findall('.//PubmedArticle'):
                title = article.findtext('.//ArticleTitle') or ""
                abstract = article.findtext('.//AbstractText') or ""
                combined = (title + " " + abstract).lower()

                for word in target_words:
                    if word in combined:
                        article_counts[word] += 1
                        # 這篇文章中出現一個詞後就不重複計算該詞
                        # 可改成集合記錄避免重複統計
    except Exception as e:
        print(f"Failed to process {file_path}: {e}")

# 處理所有檔案
for file in tqdm(files, desc="Processing PubMed files"):
    process_pubmed_file(file, article_counts)

# 輸出結果為 DataFrame
df_result = pd.DataFrame([
    {"word": word, "article_count": count}
    for word, count in article_counts.items()
])

print(df_result)

# 可選：存為 CSV
df_result.to_csv("pubmed_word_article_counts.csv", index=False)

