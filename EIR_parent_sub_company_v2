import spacy
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, udf
from pyspark.sql.types import StringType, ArrayType, MapType, IntegerType

# 初始化 Spark 和 spaCy
spark = SparkSession.builder.appName("CompanyRelationshipExtract").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# ✅ 分句函数，输出 DOCUMENT 和 sid
def split_sentences(paragraph, document_idx):
    if not isinstance(paragraph, str) or not paragraph.strip():
        return []
    try:
        doc = nlp(paragraph)
        return [
            {
                "DOCUMENT": str(document_idx),
                "sid": str(idx + 1),
                "sentence": sent.text.strip()
            }
            for idx, sent in enumerate(doc.sents) if sent.text.strip()
        ]
    except Exception:
        return []

def wrap_split_sentences(paragraph, idx):
    return split_sentences(paragraph, idx)

split_sentences_udf_spark = udf(wrap_split_sentences, ArrayType(MapType(StringType(), StringType())))

# ✅ 抽取公司关系
def extract_company_relationships(text):
    results = []
    try:
        if not isinstance(text, str) or not text.strip():
            return []

        doc = nlp(text)

        # orgs 去除排除词
        orgs = [
            (ent.text.strip(), ent.start)
            for ent in doc.ents
            if ent.label_ == "ORG" and ent.text.strip().lower() not in excluded_orgs
        ]

        lower_text = text.lower()
        matched = False

        # Pattern 1: token text 在 ownership_kw
        for token in doc:
            if token.lemma_.lower() in ownership_kw:
                # 向左看主语
                parent = sub = None
                for child in token.children:
                    if child.dep_ in {"nsubj", "nsubjpass"}:
                        for ent in doc.ents:
                            if ent.start <= child.i <= ent.end and ent.label_ == "ORG":
                                parent = ent.text
                    elif child.dep_ in {"dobj", "pobj"}:
                        for ent in doc.ents:
                            if ent.start <= child.i <= ent.end and ent.label_ == "ORG":
                                sub = ent.text
                if parent:
                    results.append({
                        "sentence": text,
                        "company": parent,
                        "role": "parent"
                    })
                    matched = True
                if sub:
                    results.append({
                        "sentence": text,
                        "company": sub,
                        "role": "subsidiary"
                    })
                    matched = True

        # Pattern 2: attr → prep → pobj
        for token in doc:
            if token.dep_ == "attr" and any(kw in token.lemma_.lower() for kw in ownership_kw):
                for child in token.children:
                    if child.dep_ == "prep":
                        for pobj in child.children:
                            if pobj.ent_type_ == "ORG":
                                results.append({
                                    "sentence": text,
                                    "company": pobj.text,
                                    "role": "subsidiary"
                                })
                                matched = True

        # Pattern 3: parent keyword in context window
        if "parent" in lower_text:
            for org_text, org_idx in orgs:
                next_tokens = " ".join([t.text.lower() for t in doc[org_idx:org_idx+5]])
                if "parent" in next_tokens:
                    results.append({
                        "sentence": text,
                        "company": org_text,
                        "role": "parent"
                    })
                    matched = True

        # 如果都没匹配到规则，仍保留 ORG 或空值
        if not matched:
            if orgs:
                for org_text, _ in orgs:
                    results.append({
                        "sentence": text,
                        "company": org_text,
                        "role": "none"
                    })
            else:
                results.append({
                    "sentence": text,
                    "company": "",
                    "role": "none"
                })

    except Exception as e:
        results.append({
            "sentence": text,
            "company": "",
            "role": f"error: {str(e)}"
        })

    return results

relationship_udf = udf(extract_company_relationships, ArrayType(MapType(StringType(), StringType())))

# ✅ 示例多列输入 DataFrame
df_paragraphs = spark.createDataFrame([
    (0, "2023-01-01", "MPI is a wholly owned subsidiary of McGuffey Inc."),
    (1, "2023-02-15", "Apple is a global tech firm."),
    (2, "2023-03-20", ""),
    (3, "2023-04-10", None),
    (4, "2023-05-12", "NVIDIA is the parent company of ARM Ltd."),
    (5, "2023-06-01", "Google acquired DeepMind."),
    (6, "2023-07-15", "No company name or relationship here.")
], ["idx", "date", "paragraph"])

# ✅ 步骤 1：拆句
df_sentences = df_paragraphs \
    .withColumn("sentences", split_sentences_udf_spark(col("paragraph"), col("idx"))) \
    .withColumn("sentence_map", explode(col("sentences"))) \
    .select(
        col("sentence_map.DOCUMENT").cast(IntegerType()).alias("DOCUMENT"),
        col("sentence_map.sid").cast(IntegerType()).alias("sid"),
        col("sentence_map.sentence").alias("text")
    )

# ✅ 步骤 2：抽取公司关系
df_result = df_sentences \
    .withColumn("relationships", relationship_udf(col("text"))) \
    .withColumn("rel", explode(col("relationships"))) \
    .select(
        col("DOCUMENT"),
        col("sid"),
        col("rel.sentence").alias("sentence"),
        col("rel.company").alias("company"),
        col("rel.role").alias("role")
    )

import ace_tools as tools
tools.display_dataframe_to_user(name="Company Relationships", dataframe=df_result.toPandas())
