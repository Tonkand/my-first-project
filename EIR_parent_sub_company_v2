import spacy
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, udf
from pyspark.sql.types import StringType, ArrayType, MapType, IntegerType

# 初始化 Spark 和 spaCy
spark = SparkSession.builder.appName("CompanyRelationshipExtract").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# ✅ 分句函数，输出 DOCUMENT 和 sid
def split_sentences(paragraph, document_idx):
    if not isinstance(paragraph, str) or not paragraph.strip():
        return []
    try:
        doc = nlp(paragraph)
        return [
            {
                "DOCUMENT": str(document_idx),
                "sid": str(idx + 1),
                "sentence": sent.text.strip()
            }
            for idx, sent in enumerate(doc.sents) if sent.text.strip()
        ]
    except Exception:
        return []

def wrap_split_sentences(paragraph, idx):
    return split_sentences(paragraph, idx)

split_sentences_udf_spark = udf(wrap_split_sentences, ArrayType(MapType(StringType(), StringType())))

import re

# ✅ 排除名单
excluded_orgs = {"fda", "us fda", "government", "agency", "llc"}

# ✅ 统一排除名单：去掉标点 + 小写
def normalize_org_name(name):
    if not name:
        return ""
    # 去除所有非字母数字
    name_clean = re.sub(r"[^A-Za-z0-9]", "", name)
    return name_clean.lower()

excluded_orgs_normalized = {normalize_org_name(org) for org in excluded_orgs}

# ✅ 抽取公司关系（整合 normalized 排除逻辑）
def extract_company_relationships(text):
    results = []
    try:
        if not isinstance(text, str) or not text.strip():
            return []

        doc = nlp(text)

        orgs = []
        for ent in doc.ents:
            if ent.label_ == "ORG":
                norm_name = normalize_org_name(ent.text.strip())
                if norm_name not in excluded_orgs_normalized:
                    orgs.append((ent.text.strip(), ent.start))

        lower_text = text.lower()
        matched = False

        # Pattern 1: 动词 children
        ownership_kw = {
            "owner", "acquire", "buy", "purchase", "take over",
            "gain control", "subsidiary", "controlled", "owned"
        }
        for token in doc:
            if token.lemma_.lower() in ownership_kw:
                parent = sub = None
                for child in token.children:
                    if child.dep_ in {"nsubj", "nsubjpass"}:
                        for ent in doc.ents:
                            if ent.start <= child.i <= ent.end and ent.label_ == "ORG":
                                norm_name = normalize_org_name(ent.text)
                                if norm_name not in excluded_orgs_normalized:
                                    parent = ent.text
                    elif child.dep_ in {"dobj", "pobj"}:
                        for ent in doc.ents:
                            if ent.start <= child.i <= ent.end and ent.label_ == "ORG":
                                norm_name = normalize_org_name(ent.text)
                                if norm_name not in excluded_orgs_normalized:
                                    sub = ent.text
                if parent:
                    results.append({
                        "sentence": text,
                        "company": parent,
                        "role": "parent"
                    })
                    matched = True
                if sub:
                    results.append({
                        "sentence": text,
                        "company": sub,
                        "role": "subsidiary"
                    })
                    matched = True

        # Pattern 4: attr → prep → pobj
        for token in doc:
            if token.dep_ == "attr" and any(kw in token.lemma_.lower() for kw in ownership_kw):
                for child in token.children:
                    if child.dep_ == "prep":
                        for pobj in child.children:
                            if pobj.ent_type_ == "ORG":
                                norm_name = normalize_org_name(pobj.text)
                                if norm_name not in excluded_orgs_normalized:
                                    results.append({
                                        "sentence": text,
                                        "company": pobj.text,
                                        "role": "subsidiary"
                                    })
                                    matched = True

        # Pattern 3: parent keyword in context window
        if "parent" in lower_text:
            for org_text, org_idx in orgs:
                next_tokens = " ".join([t.text.lower() for t in doc[org_idx:org_idx+5]])
                if "parent" in next_tokens:
                    results.append({
                        "sentence": text,
                        "company": org_text,
                        "role": "parent"
                    })
                    matched = True

        # 若没有匹配到任何模式
        if not matched:
            if orgs:
                for org_text, _ in orgs:
                    results.append({
                        "sentence": text,
                        "company": org_text,
                        "role": "none"
                    })
            else:
                results.append({
                    "sentence": text,
                    "company": "",
                    "role": "none"
                })

    except Exception as e:
        results.append({
            "sentence": text,
            "company": "",
            "role": f"error: {str(e)}"
        })

    return results

relationship_udf = udf(extract_company_relationships, ArrayType(MapType(StringType(), StringType())))

# ✅ 示例多列输入 DataFrame
df_paragraphs = spark.createDataFrame([
    (0, "2023-01-01", "MPI is a wholly owned subsidiary of McGuffey Inc."),
    (1, "2023-02-15", "Apple is a global tech firm."),
    (2, "2023-03-20", ""),
    (3, "2023-04-10", None),
    (4, "2023-05-12", "NVIDIA is the parent company of ARM Ltd."),
    (5, "2023-06-01", "Google acquired DeepMind."),
    (6, "2023-07-15", "No company name or relationship here.")
], ["idx", "date", "paragraph"])

# ✅ 步骤 1：拆句
df_sentences = df_paragraphs \
    .withColumn("sentences", split_sentences_udf_spark(col("paragraph"), col("idx"))) \
    .withColumn("sentence_map", explode(col("sentences"))) \
    .select(
        col("sentence_map.DOCUMENT").cast(IntegerType()).alias("DOCUMENT"),
        col("sentence_map.sid").cast(IntegerType()).alias("sid"),
        col("sentence_map.sentence").alias("text")
    )

# ✅ 步骤 2：抽取公司关系
df_result = df_sentences \
    .withColumn("relationships", relationship_udf(col("text"))) \
    .withColumn("rel", explode(col("relationships"))) \
    .select(
        col("DOCUMENT"),
        col("sid"),
        col("rel.sentence").alias("sentence"),
        col("rel.company").alias("company"),
        col("rel.role").alias("role")
    )

import ace_tools as tools
tools.display_dataframe_to_user(name="Company Relationships", dataframe=df_result.toPandas())
