import spacy
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType, StructField, StringType, ArrayType

spark = SparkSession.builder.getOrCreate()
nlp = spacy.load("en_core_web_sm")

raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]

def process_text(text):
    doc = nlp(text)
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]

    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]

    match_keyword = any(l in raw_keywords for l in lemmas)
    match_noun = any(n in noun_terms for n in lemmas)
    match_custom = next((k for k in custom_keywords if k in text.lower()), None)

    flag = "cata" if (match_keyword or match_noun) and match_custom else ""
    voice = "passive" if any(dep == "nsubjpass" for _, _, _, dep, _ in tokens) else "active" if any(dep == "nsubj" for _, _, _, dep, _ in tokens) else "unknown"
    return (flag, match_custom, voice)

# 定義輸出 schema
schema = StructType([
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType())
])

process_text_udf = udf(process_text, schema)

# 測試資料轉為 Spark DataFrame
text_data = [
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",)
]
df_spark = spark.createDataFrame(text_data, ["text"])

# 套用 UDF
df_result = df_spark.withColumn("result", process_text_udf(col("text"))) \
    .select("text", "result.flag", "result.closest_term", "result.voice")

df_result.show(truncate=False)

#######version 2
import spacy
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, lit
from pyspark.sql.types import StructType, StructField, StringType, ArrayType

# 初始化 spaCy 模型（只在 driver node 上跑）
nlp = spacy.load("en_core_web_sm")

# 建立 Spark session
spark = SparkSession.builder.appName("SpaCy NLP Flags").getOrCreate()

# 規則建構（broadcast）
def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

# Broadcast 所有參數
raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]
flag_label = "cata"
custom_mode = "keyword"  # 可為 year, hour, keyword
require_both_keyword_and_noun = False
ORG_mode = True
hours_pattern = r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b'
match_rules = create_match_rules(raw_keywords, noun_terms)

br_match_rules = spark.sparkContext.broadcast(match_rules)
br_flag_label = spark.sparkContext.broadcast(flag_label)
br_custom_keywords = spark.sparkContext.broadcast(custom_keywords)
br_ORG_mode = spark.sparkContext.broadcast(ORG_mode)
br_require_both = spark.sparkContext.broadcast(require_both_keyword_and_noun)

# 定義 UDF 處理
def analyze_text(text):
    if not text:
        return ("", None, "unknown", None)

    doc = nlp(text)
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]

    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]

    match_rules = br_match_rules.value
    custom_keywords = br_custom_keywords.value
    flag_label = br_flag_label.value
    require_both = br_require_both.value
    ORG_mode = br_ORG_mode.value

    important_indices = []
    match_keyword = False
    match_noun = False
    match_org = False
    excluded_subjects = {"government", "ministry", "department", "agency", "council"}

    for text_tok, lemma, pos, _, idx in tokens:
        for rule in match_rules:
            if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                important_indices.append(idx)
                match_noun = True

    for rule in match_rules:
        if rule["type"] == "phrasal":
            for i in range(len(tokens) - 1):
                if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                    important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                    match_keyword = True

    if ORG_mode:
        for org_text, label, start in ents:
            if label == "ORG" and org_text.lower() not in excluded_subjects:
                match_org = True
                important_indices.append(start)

        if not match_org:
            fallback_orgs = ["apple", "microsoft", "tesla", "amazon", "google"]
            for token in token_texts:
                if token in fallback_orgs:
                    match_org = True

    has_excluded_subject = not match_noun and any(
        lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
        for _, lemma, _, dep, _ in tokens
    )

    matched_terms = [kw for kw in custom_keywords if kw in text.lower()]
    has_time = bool(matched_terms)

    closest_time = None
    if matched_terms and important_indices:
        def distance(kw):
            return min(
                abs(t.i - idx)
                for idx in important_indices
                for t in doc
                if t.text.lower() in kw.split()
            ) if any(w in text.lower() for w in kw.split()) else float('inf')

        closest_time = min(matched_terms, key=distance)

    voice = "unknown"
    for _, _, _, dep, _ in tokens:
        if dep == "nsubjpass":
            voice = "passive"
            break
        elif dep == "nsubj":
            voice = "active"

    if require_both:
        passed = match_keyword and (match_noun or match_org) if ORG_mode else match_keyword and match_noun
    else:
        passed = match_keyword or match_noun or (match_org if ORG_mode else False)

    if passed and has_time and not has_excluded_subject:
        return (flag_label, closest_time, voice, matched_terms[0] if matched_terms else None)
    else:
        return ("", None, voice, matched_terms[0] if matched_terms else None)

# 回傳格式
schema = StructType([
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType()),
    StructField("matched_term", StringType())
])

analyze_udf = udf(analyze_text, schema)

# 輸入資料
df = spark.createDataFrame([
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",)
], ["text"])

# 套用 UDF
df_result = df.withColumn("result", analyze_udf(col("text"))) \
    .select("text", "result.flag", "result.closest_term", "result.voice", "result.matched_term")

df_result.display()

######version 3
import spacy
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark and spaCy
spark = SparkSession.builder.appName("Full NLP Pipeline with spaCy").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# Define rules
raw_keywords = ["acquire", "gain", "purchase", "buy", "take over"]
noun_terms = ["firm", "startup", "company", "site"]
custom_keywords = ["soon", "immediately", "right away", "as soon as possible", "later"]
flag_label = "cata"
custom_mode = "keyword"  # Options: 'year', 'hour', 'keyword'
require_both_keyword_and_noun = False
ORG_mode = True
hours_pattern = r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b'

# Create match rules
def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

# Broadcast configurations
match_rules = create_match_rules(raw_keywords, noun_terms)
br_match_rules = spark.sparkContext.broadcast(match_rules)
br_flag_label = spark.sparkContext.broadcast(flag_label)
br_custom_keywords = spark.sparkContext.broadcast(custom_keywords)
br_custom_mode = spark.sparkContext.broadcast(custom_mode)
br_hours_pattern = spark.sparkContext.broadcast(hours_pattern)
br_ORG_mode = spark.sparkContext.broadcast(ORG_mode)
br_require_both = spark.sparkContext.broadcast(require_both_keyword_and_noun)

# Define UDF
def analyze_text(text):
    if not text:
        return ("", None, "unknown", None)

    doc = nlp(text)
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]

    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]

    match_rules = br_match_rules.value
    custom_keywords = br_custom_keywords.value
    flag_label = br_flag_label.value
    require_both = br_require_both.value
    ORG_mode = br_ORG_mode.value
    custom_mode = br_custom_mode.value
    hours_pattern = br_hours_pattern.value

    important_indices = []
    match_keyword = False
    match_noun = False
    match_org = False
    excluded_subjects = {"government", "ministry", "department", "agency", "council"}

    for text_tok, lemma, pos, _, idx in tokens:
        for rule in match_rules:
            if rule["type"] == "lemma" and lemma == rule["value"] and pos in ["VERB", "NOUN"]:
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "compound" and (text_tok == rule["value"] or lemma == rule["value"]):
                important_indices.append(idx)
                match_keyword = True
            elif rule["type"] == "keyword" and (lemma == rule["value"] or text_tok == rule["value"]):
                important_indices.append(idx)
                match_noun = True

    for rule in match_rules:
        if rule["type"] == "phrasal":
            for i in range(len(tokens) - 1):
                if (lemmas[i], token_texts[i + 1]) == rule["value"]:
                    important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                    match_keyword = True

    if ORG_mode:
        for org_text, label, start in ents:
            if label == "ORG" and org_text.lower() not in excluded_subjects:
                match_org = True
                important_indices.append(start)
        if not match_org:
            fallback_orgs = ["apple", "microsoft", "tesla", "amazon", "google"]
            for token in token_texts:
                if token in fallback_orgs:
                    match_org = True

    has_excluded_subject = not match_noun and any(
        lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
        for _, lemma, _, dep, _ in tokens
    )

    if custom_mode == "year":
        time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", text)
    elif custom_mode == "hour":
        time_matches = re.findall(hours_pattern, text, re.IGNORECASE)
        time_matches = [''.join(x).strip() if isinstance(x, tuple) else x.strip() for x in time_matches]
    elif custom_mode == "keyword":
        time_matches = [kw for kw in custom_keywords if kw in text.lower()]
    else:
        time_matches = []

    has_time = bool(time_matches)

    closest_time = None
    if time_matches and important_indices:
        def distance(t):
            return min(
                abs(tk.i - idx)
                for idx in important_indices
                for tk in doc
                if tk.text.lower() in t.lower().split()
            ) if any(w in text.lower() for w in t.lower().split()) else float('inf')

        closest_time = min(time_matches, key=distance)

    voice = "unknown"
    for _, _, _, dep, _ in tokens:
        if dep == "nsubjpass":
            voice = "passive"
            break
        elif dep == "nsubj":
            voice = "active"

    if require_both:
        passed = match_keyword and (match_noun or match_org) if ORG_mode else match_keyword and match_noun
    else:
        passed = match_keyword or match_noun or (match_org if ORG_mode else False)

    if passed and has_time and not has_excluded_subject:
        return (flag_label, closest_time, voice, time_matches[0] if time_matches else None)
    else:
        return ("", None, voice, time_matches[0] if time_matches else None)

schema = StructType([
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType()),
    StructField("matched_term", StringType())
])

analyze_udf = udf(analyze_text, schema)

# Test data
df = spark.createDataFrame([
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",)
], ["text"])

df_result = df.withColumn("result", analyze_udf(col("text"))) \
    .select("text", "result.flag", "result.closest_term", "result.voice", "result.matched_term")

import ace_tools as tools; tools.display_dataframe_to_user(name="NLP Flagging Result", dataframe=df_result.toPandas())

#### version 4
import spacy
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark and spaCy
spark = SparkSession.builder.appName("MultiRule NLP").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# Define search configuration
search_config = [
    {
        "custom_mode": "hour",
        "raw_keywords": ["find", "get"],
        "noun_terms": ["firm"],
        "flag_label": "rule1",
        "ORG_mode": True,
        "require_both": False
    },
    {
        "custom_mode": "year",
        "raw_keywords": ["move"],
        "noun_terms": ["facility"],
        "flag_label": "rule2",
        "ORG_mode": True,
        "require_both": True
    },
    {
        "custom_mode": "keyword",
        "raw_keywords": ["acquire", "take over"],
        "noun_terms": ["company"],
        "custom_keywords": ["soon", "immediately", "as soon as possible"],
        "flag_label": "rule3",
        "ORG_mode": False,
        "require_both": False
    }
]

br_config = spark.sparkContext.broadcast(search_config)

def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

def analyze_with_rules(text):
    if not text:
        return ("", None, "unknown", "", "")

    doc = nlp(text)
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]
    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]
    fallback_orgs = {"apple", "tesla", "microsoft", "google", "amazon"}
    excluded_subjects = {"government", "ministry", "department", "agency", "council"}

    for rule in br_config.value:
        match_rules = create_match_rules(rule["raw_keywords"], rule.get("noun_terms", []))
        important_indices = []
        match_keyword = False
        match_noun = False
        match_org = False

        for text_tok, lemma, pos, _, idx in tokens:
            for r in match_rules:
                if r["type"] == "lemma" and lemma == r["value"] and pos in ["VERB", "NOUN"]:
                    important_indices.append(idx)
                    match_keyword = True
                elif r["type"] == "compound" and (text_tok == r["value"] or lemma == r["value"]):
                    important_indices.append(idx)
                    match_keyword = True
                elif r["type"] == "keyword" and (lemma == r["value"] or text_tok == r["value"]):
                    important_indices.append(idx)
                    match_noun = True

        for r in match_rules:
            if r["type"] == "phrasal":
                for i in range(len(tokens) - 1):
                    if (lemmas[i], token_texts[i + 1]) == r["value"]:
                        important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                        match_keyword = True

        if rule.get("ORG_mode", False):
            for org_text, label, start in ents:
                if label == "ORG" and org_text.lower() not in excluded_subjects:
                    match_org = True
                    important_indices.append(start)
            if not match_org:
                for token in token_texts:
                    if token in fallback_orgs:
                        match_org = True

        has_excluded_subject = not match_noun and any(
            lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
            for _, lemma, _, dep, _ in tokens
        )

        custom_mode = rule["custom_mode"]
        time_matches = []

        if custom_mode == "year":
            time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", text)
        elif custom_mode == "hour":
            time_matches = re.findall(r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b', text, re.IGNORECASE)
            time_matches = [t.strip() for t in time_matches]
        elif custom_mode == "keyword":
            keywords = rule.get("custom_keywords", [])
            time_matches = [kw for kw in keywords if kw in text.lower()]

        if time_matches and important_indices:
            def distance(t):
                return min(
                    abs(tk.i - idx)
                    for idx in important_indices
                    for tk in doc
                    if tk.text.lower() in t.lower().split()
                ) if any(w in text.lower() for w in t.lower().split()) else float('inf')

            closest_time = min(time_matches, key=distance)
        else:
            closest_time = None

        voice = "unknown"
        for _, _, _, dep, _ in tokens:
            if dep == "nsubjpass":
                voice = "passive"
                break
            elif dep == "nsubj":
                voice = "active"

        require_both = rule.get("require_both", True)
        if require_both:
            passed = match_keyword and (match_noun or match_org)
        else:
            passed = match_keyword or match_noun or (match_org if rule.get("ORG_mode") else False)

        if passed and time_matches and not has_excluded_subject:
            return (rule["flag_label"], closest_time, voice, custom_mode, time_matches[0])

    return ("", None, "unknown", "", "")

# Define schema and UDF
schema = StructType([
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType()),
    StructField("mode", StringType()),
    StructField("matched_term", StringType())
])
analyze_udf = udf(analyze_with_rules, schema)

# Input data
df = spark.createDataFrame([
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",),
    ("We need to find a firm by 10am.",),
    ("They plan to move the facility in 2022.",)
], ["text"])

df_result = df.withColumn("result", analyze_udf(col("text"))) \
    .select("text", "result.flag", "result.closest_term", "result.voice", "result.mode", "result.matched_term")

import ace_tools as tools; tools.display_dataframe_to_user(name="MultiRule NLP Result", dataframe=df_result.toPandas())


#####version 5

# Re-execute after environment reset
import spacy
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StructType, StructField, StringType

# Initialize Spark and spaCy
spark = SparkSession.builder.appName("MultiRule NLP").getOrCreate()
nlp = spacy.load("en_core_web_sm")

# Define search configuration
search_config = [
    {
        "custom_mode": "hour",
        "raw_keywords": ["find", "get"],
        "noun_terms": ["firm"],
        "flag_label": "rule1",
        "ORG_mode": True,
        "require_both": False
    },
    {
        "custom_mode": "year",
        "raw_keywords": ["move"],
        "noun_terms": ["facility"],
        "flag_label": "rule2",
        "ORG_mode": True,
        "require_both": True
    },
    {
        "custom_mode": "keyword",
        "raw_keywords": ["acquire", "take over"],
        "noun_terms": ["company"],
        "custom_keywords": ["soon", "immediately", "as soon as possible"],
        "flag_label": "rule3",
        "ORG_mode": False,
        "require_both": False
    }
]

br_config = spark.sparkContext.broadcast(search_config)

def create_match_rules(word_list, noun_terms=None):
    word_list = word_list + (noun_terms or [])
    match_rules = []
    noun_terms = set(noun_terms or [])
    for item in word_list:
        item = item.strip().lower()
        if item in noun_terms:
            match_rules.append({"type": "keyword", "value": item})
        elif " " in item:
            parts = item.split()
            if len(parts) == 2:
                match_rules.append({"type": "phrasal", "value": (parts[0], parts[1])})
        elif "-" in item or item.endswith("out") or item.endswith("over"):
            match_rules.append({"type": "compound", "value": item})
        else:
            match_rules.append({"type": "lemma", "value": item})
    return match_rules

def analyze_with_rules(text):
    if not text:
        return ("", None, "unknown", "", "")

    doc = nlp(text)
    tokens = [(t.text, t.lemma_, t.pos_, t.dep_, t.i) for t in doc]
    ents = [(e.text, e.label_, e.start) for e in doc.ents]
    token_texts = [t[0].lower() for t in tokens]
    lemmas = [t[1].lower() for t in tokens]
    fallback_orgs = {"apple", "tesla", "microsoft", "google", "amazon"}
    excluded_subjects = {"government", "ministry", "department", "agency", "council"}

    for rule in br_config.value:
        match_rules = create_match_rules(rule["raw_keywords"], rule.get("noun_terms", []))
        important_indices = []
        match_keyword = False
        match_noun = False
        match_org = False

        for text_tok, lemma, pos, _, idx in tokens:
            for r in match_rules:
                if r["type"] == "lemma" and lemma == r["value"] and pos in ["VERB", "NOUN"]:
                    important_indices.append(idx)
                    match_keyword = True
                elif r["type"] == "compound" and (text_tok == r["value"] or lemma == r["value"]):
                    important_indices.append(idx)
                    match_keyword = True
                elif r["type"] == "keyword" and (lemma == r["value"] or text_tok == r["value"]):
                    important_indices.append(idx)
                    match_noun = True

        for r in match_rules:
            if r["type"] == "phrasal":
                for i in range(len(tokens) - 1):
                    if (lemmas[i], token_texts[i + 1]) == r["value"]:
                        important_indices.extend([tokens[i][4], tokens[i + 1][4]])
                        match_keyword = True

        if rule.get("ORG_mode", False):
            for org_text, label, start in ents:
                if label == "ORG" and org_text.lower() not in excluded_subjects:
                    match_org = True
                    important_indices.append(start)
            if not match_org:
                for token in token_texts:
                    if token in fallback_orgs:
                        match_org = True

        has_excluded_subject = not match_noun and any(
            lemma in excluded_subjects and dep in {"nsubj", "nsubjpass"}
            for _, lemma, _, dep, _ in tokens
        )

        custom_mode = rule["custom_mode"]
        time_matches = []

        if custom_mode == "year":
            time_matches = re.findall(r"\b(19[0-9]{2}|20[0-9]{2})\b", text)
        elif custom_mode == "hour":
            time_matches = re.findall(r'\b(?:\d{1,2}(?::\d{2})?\s*(?:am|pm))\b', text, re.IGNORECASE)
            time_matches = [t.strip() for t in time_matches]
        elif custom_mode == "keyword":
            keywords = rule.get("custom_keywords", [])
            time_matches = [kw for kw in keywords if kw in text.lower()]

        if time_matches and important_indices:
            def distance(t):
                return min(
                    abs(tk.i - idx)
                    for idx in important_indices
                    for tk in doc
                    if tk.text.lower() in t.lower().split()
                ) if any(w in text.lower() for w in t.lower().split()) else float('inf')

            closest_time = min(time_matches, key=distance)
        else:
            closest_time = None

        voice = "unknown"
        for _, _, _, dep, _ in tokens:
            if dep == "nsubjpass":
                voice = "passive"
                break
            elif dep == "nsubj":
                voice = "active"

        require_both = rule.get("require_both", True)
        if require_both:
            passed = match_keyword and (match_noun or match_org)
        else:
            passed = match_keyword or match_noun or (match_org if rule.get("ORG_mode") else False)

        if passed and time_matches and not has_excluded_subject:
            return (rule["flag_label"], closest_time, voice, custom_mode, time_matches[0])

    return ("", None, "unknown", "", "")

# Define schema and UDF
schema = StructType([
    StructField("flag", StringType()),
    StructField("closest_term", StringType()),
    StructField("voice", StringType()),
    StructField("mode", StringType()),
    StructField("matched_term", StringType())
])
analyze_udf = udf(analyze_with_rules, schema)

# Input data
df = spark.createDataFrame([
    ("We will acquire the company as soon as possible.",),
    ("The firm bought the site in 2020.",),
    ("Tesla gained control right away and expanded later.",),
    ("They will act immediately to take over.",),
    ("Government took possession without notice.",),
    ("We need to find a firm by 10am.",),
    ("They plan to move the facility in 2022.",)
], ["text"])

df_result = df.withColumn("result", analyze_udf(col("text"))) \
    .select("text", "result.flag", "result.closest_term", "result.voice", "result.mode", "result.matched_term")

import ace_tools as tools; tools.display_dataframe_to_user(name="MultiRule NLP Result", dataframe=df_result.toPandas())

